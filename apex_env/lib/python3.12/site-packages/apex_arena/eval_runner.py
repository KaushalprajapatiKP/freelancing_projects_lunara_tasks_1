import asyncio
import concurrent
import contextlib
import datetime
import json
import logging
import os
import socket
import statistics
import subprocess
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from dataclasses import asdict, dataclass
from datetime import timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import httpx
from rich.console import Console
from rich.live import Live
from rich.markup import escape
from rich.progress import Progress
from tqdm import tqdm

from .agents import MCPAgent, TerminusV1Agent, TerminusV2Agent
from .cli import build_docker_image
from .llm import LLMClient
from .result_streamer import ResultStreamer, extract_score_from_grade_result
from .tester import ApexArenaClient

logger = logging.getLogger(__name__)
console = Console()


class LoggingApexArenaClient(ApexArenaClient):
    def __init__(
        self,
        server_url: str,
        model: str,
        max_tokens: int,
        run_id: str,
        transcript_file: Path,
        max_turns: int = 50,
        # New streaming parameters
        streamer: Optional[ResultStreamer] = None,
        evaluation_id: Optional[str] = None,
        rollout_id: Optional[str] = None,
        # Task version tracking
        task_version_id: Optional[str] = None,
        # Computer use
        enable_computer_use: bool = False,
        # Agent type
        agent_type: Optional[str] = None,
    ):
        super().__init__(server_url, model, max_tokens)
        self.run_id = run_id
        self.transcript_file = transcript_file
        self.transcript_log = []
        self.max_turns = max_turns

        # Streaming configuration
        self.streamer = streamer
        self.evaluation_id = evaluation_id
        self.rollout_id = rollout_id
        self.conversation_sequence = 0  # Separate counter for conversation messages
        self.task_version_id = task_version_id

        # Initialize the appropriate agent based on explicit selection or model
        proxy_client = LLMClient()

        # Determine which agent to use
        if agent_type:
            # Explicit agent selection
            selected_agent_type = agent_type
        else:
            # Auto-routing based on model
            # starburst and starburst-plus use terminus_v1, all others default to mcp
            if model in ("starburst", "starburst-plus"):
                selected_agent_type = "terminus_v1"
            else:
                selected_agent_type = "mcp"

        # Initialize the selected agent
        if selected_agent_type == "terminus_v1":
            self.agent = TerminusV1Agent(
                proxy_client=proxy_client,
                model=model,
                max_tokens=max_tokens,
                max_turns=max_turns,
                enable_computer_use=enable_computer_use,
            )
            self.agent_type = "TerminusV1Agent"
        elif selected_agent_type == "terminus_v2":
            self.agent = TerminusV2Agent(
                proxy_client=proxy_client,
                model=model,
                max_tokens=max_tokens,
                max_turns=max_turns,
                enable_computer_use=enable_computer_use,
            )
            self.agent_type = "TerminusV2Agent"
        elif selected_agent_type == "mcp":
            self.agent = MCPAgent(
                proxy_client=proxy_client,
                model=model,
                max_tokens=max_tokens,
                max_turns=max_turns,
                enable_computer_use=enable_computer_use,
            )
            self.agent_type = "MCPAgent"
        else:
            raise ValueError(f"Unknown agent type: {selected_agent_type}")

    def log_message(self, message_type: str, content: str):
        """Log a message with timestamp and run ID."""
        timestamp = datetime.datetime.now(datetime.UTC).isoformat()
        log_entry = {
            "timestamp": timestamp,
            "run_id": self.run_id,
            "type": message_type,
            "content": content,
        }
        self.transcript_log.append(log_entry)
        # Print directly to console with run ID
        console.print(
            f"[bold cyan]{self.run_id}[/bold cyan] [blue]{message_type}:[/blue] {escape(content[:300])}"
        )

        # Stream message if streaming is enabled and it's a conversation message
        if self.streamer and self.evaluation_id and self.rollout_id:
            # Map message type to OpenAI role
            role = self._map_message_type_to_role(message_type)

            # Only stream messages that have roles (actual conversation messages)
            if role is not None:
                message_data = {
                    "timestamp": timestamp,
                    "sequence_number": self.conversation_sequence,
                    "role": role,
                    "content": content,
                }

                self.conversation_sequence += 1

                # Create async task to stream message (fire and forget)
                asyncio.create_task(self._stream_message_async(message_data))

    def _map_message_type_to_role(self, message_type: str) -> Optional[str]:
        """Map apex-arena message types to OpenAI role format."""
        role_mapping = {
            "PROMPT": "user",
            "MODEL_TEXT": "assistant",
            "THINKING": "assistant",
            "TOOL_CALL": "assistant",
            "TOOL_RESULT": "tool",
            # Non-conversation messages don't have roles (will be filtered out)
            "SETUP": None,
            "MCP": None,
            "TOOLS": None,
            "TURN": None,
            "COMPLETE": None,
            "ERROR": None,
            "ANSWER": None,
            "NO_TOOLS": None,
            "RETRY": None,
            "EXCEPTION": None,
            "GRADE": None,
        }
        return role_mapping.get(message_type)

    async def _stream_message_async(self, message_data: Dict[str, Any]):
        """Async helper to stream a single message."""
        max_retries = 5
        retry = 0
        while retry < max_retries:
            try:
                await self.streamer.stream_messages(
                    self.evaluation_id, self.rollout_id, [message_data]
                )
                break
            except Exception as e:
                retry += 1
                # Don't let streaming errors crash the evaluation
                console.print(
                    f"[yellow]Warning: Failed to stream message (Attempt {retry}/{max_retries}): {type(e)}: {e}[/yellow]"
                )

    def save_transcript(self):
        """Save the complete transcript to file."""
        transcript_data = {
            "run_id": self.run_id,
            "model": self.model,
            "max_tokens": self.max_tokens,
            "transcript": self.transcript_log,
        }

        # Include task_version_id if available
        if self.task_version_id is not None:
            transcript_data["task_version_id"] = self.task_version_id

        with open(self.transcript_file, "w") as f:
            json.dump(transcript_data, f, indent=2)

    async def run_problem(self, problem_id: str) -> Dict[str, Any]:
        """Override to add logging."""
        self.log_message("SETUP", f"Starting problem: {problem_id}")
        self.log_message("AGENT", f"Using {self.agent_type} for model: {self.model}")

        try:
            result = await self._run_problem_with_logging(problem_id)
            self.log_message(
                "COMPLETE", f"Finished with grade: {result.get('grade_result', 'N/A')}"
            )
            return result
        except Exception as e:
            import traceback

            error_details = (
                f"Failed with error: {str(e)}\nTraceback: {traceback.format_exc()}"
            )
            self.log_message("ERROR", error_details)
            raise
        finally:
            self.save_transcript()

    async def _run_problem_with_logging(self, problem_id: str) -> Dict[str, Any]:
        """Run problem using the appropriate agent with detailed logging."""
        from mcp import ClientSession
        from mcp.client.streamable_http import streamablehttp_client

        # Connect to the streamable HTTP server with retries
        max_connection_retries = 5

        # Add a small initial delay to help with server readiness
        if max_connection_retries > 1:
            await asyncio.sleep(1)

        logger.debug(f"[DEBUG] {self.server_url}: running problem {problem_id}")

        for connection_attempt in range(max_connection_retries):
            try:
                async with streamablehttp_client(
                    self.server_url,
                    timeout=timedelta(
                        seconds=7200
                    ),  # 1 hour - tool-level timeouts handle actual limits
                    sse_read_timeout=timedelta(seconds=7200),
                ) as (
                    read_stream,
                    write_stream,
                    _,
                ):
                    # Create a session using the client streams
                    async with ClientSession(read_stream, write_stream) as session:
                        # Initialize the connection
                        await session.initialize()
                        self.log_message("MCP", "Connected to MCP server")

                        # Run the problem using the appropriate agent
                        result = await self.agent.run_problem(
                            problem_id=problem_id, session=session, logger_client=self
                        )

                        return result

            except Exception as e:
                if connection_attempt < max_connection_retries - 1:
                    self.log_message(
                        "RETRY",
                        f"{self.server_url}: MCP error (attempt {connection_attempt + 1}/{max_connection_retries}): {str(e)}, {type(e)}.",
                    )
                    print("=== MCP error===")
                    traceback.print_exception(e)
                    await asyncio.sleep(5)
                    continue
                else:
                    self.log_message(
                        "ERROR",
                        f"MCP connection failed after {max_connection_retries} attempts. Root error: {str(e)}",
                    )
                    print("=== MCP error===")
                    traceback.print_exception(e)
                    raise

        # This should never be reached due to break/raise above, but just in case
        raise RuntimeError("Unexpected end of retry loop")


class ProgressManager:
    def __init__(self, use_live: bool = True):
        self.progress = None
        self.task_id = None
        self.live = None
        self.use_live = use_live

    def start_progress(self, total: int):
        self.progress = Progress()
        self.task_id = self.progress.add_task(
            "[green]Running evaluations...", total=total
        )

        if self.use_live:
            # Use Live to pin the progress bar at the top
            self.live = Live(self.progress, console=console, refresh_per_second=4)
            self.live.start()
        else:
            # Simple progress bar for debugging
            self.progress.start()

    def update_progress(self, advance: int = 1):
        if self.progress and self.task_id is not None:
            self.progress.update(self.task_id, advance=advance)

    def stop_progress(self):
        if self.live:
            self.live.stop()
        elif self.progress:
            self.progress.stop()


# Global progress manager for the evaluation
eval_progress = ProgressManager()


@dataclass
class EvalResult:
    task: str
    run: int
    grade_result: Any
    duration: float
    success: bool
    error: Optional[str] = None
    transcript_file: Optional[str] = None
    server_log_file: Optional[str] = None
    entrypoint_log_file: Optional[str] = None


@dataclass
class AggregatedResult:
    task: str
    mean_score: float
    std_dev: float
    success_rate: float
    total_runs: int
    scores: List[float]
    coefficient_of_variation: float


def _allocate_port() -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("", 0))  # 0 ‚áí ‚Äúpick any free port‚Äù
        return s.getsockname()[1]


class DockerManager:
    def __init__(self):
        self.running_containers: Set[str] = set()
        self.client = httpx.Client(timeout=10.0)

    def wait_for_server_ready(
        self, server_url: str, container_name: str, max_attempts: int = 360
    ) -> bool:
        """Wait for MCP server to be ready by making health check requests.

        Waits up to 10 minutes: first 10 seconds with 5s intervals, then 10s intervals.
        """

        console.print(
            "[cyan]‚è≥ Waiting for server to start (may take up to 10 minutes)...[/cyan]"
        )
        console.print(
            f"[dim]You can check logs with: docker logs -f {container_name}[/dim]"
        )

        # Show internet access status
        no_internet = os.getenv("NO_INTERNET", "true").lower()
        is_restricted = no_internet not in ["false", "f", "0"]
        if is_restricted:
            console.print("[yellow]‚ö† INTERNET IS OFF in this session[/yellow]")
        else:
            console.print("[green]‚úì INTERNET IS ALLOWED in this session[/green]")

        attempt = 0

        while attempt < max_attempts:
            attempt += 1
            try:
                logger.debug(f"[DEBUG]: Sending health check request to {server_url}")
                # Try a simple GET request to the MCP server URL
                response = self.client.get(server_url)
                logger.debug(f"[DEBUG]: Got response {response} from {server_url}")
                if response.status_code in [
                    200,
                    404,
                    405,
                    406,
                    307,
                ]:  # Any valid HTTP response means server is up
                    console.print(
                        f"[green]‚úì Server ready at {server_url} (attempt {attempt})[/green]"
                    )
                    return True
            except Exception:
                # Server not ready yet, continue waiting
                console.print(
                    f"[dim]Server not ready yet at {server_url} (attempt {attempt}), be patient...[/dim]"
                )

            if attempt < 5:
                time.sleep(5)
            else:
                time.sleep(10)

        console.print(
            f"[red]‚úó Server failed to become ready at {server_url} after 60 minutes ({max_attempts} attempts)[/red]"
        )
        return False

    def start_container(
        self,
        task: str,
        run: int,
        port: int,
        server_log_file: Path,
        image_name: Optional[str] = None,
        container_name: Optional[str] = None,
        cpus: float = 4.0,
    ) -> str:
        """Start a Docker container for the task and return container name."""
        if container_name is None:
            timestamp = int(time.time())
            container_name = f"apex-arena-eval-{task}-{run}-{timestamp}"

        assert image_name is not None, "Image name is required"

        # Create the log file if it doesn't exist
        server_log_file.touch()

        # Check if internet should be restricted (default: true)
        no_internet = os.getenv("NO_INTERNET", "true").lower()
        restrict_network = no_internet not in ["false", "f", "0"]

        cmd = [
            "docker",
            "run",
            "-d",
            "--name",
            container_name,
            "--privileged",
            "--cgroupns=private",
            "--cpus",
            str(cpus),
        ]

        # Start container on default bridge network (with internet for bootstrap)
        # We'll attach restricted_net after bootstrap completes
        cmd.extend(
            [
                "-p",
                f"{port}:8001",
                "-v",
                f"{Path.cwd()}/tasks:/mcp_server/tasks",
                "-v",
                f"{server_log_file.parent.absolute()}:/mcp_server/logs",
                "-e",
                "MCP_TESTING_MODE=1",
                image_name,
                "sh",
                "-c",
                f"python3 /mcp_server/apex_arena/server.py > /mcp_server/logs/{server_log_file.name} 2>&1",
            ]
        )

        logger.debug(f"[DEBUG] Starting container {image_name}")

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError(f"Failed to start container: {result.stderr}")
        self.running_containers.add(container_name)

        # Start streaming docker logs to capture full entrypoint output
        entrypoint_log_file = (
            server_log_file.parent / f"{server_log_file.stem}-entrypoint.log"
        )
        try:
            log_stream = open(entrypoint_log_file, "w")
            # Start docker logs -f in background to capture all container output
            subprocess.Popen(
                ["docker", "logs", "-f", container_name],
                stdout=log_stream,
                stderr=subprocess.STDOUT,
            )
            logger.debug(
                f"[DEBUG] Started streaming docker logs to {entrypoint_log_file}"
            )
        except Exception as e:
            logger.warning(f"Failed to start docker logs streaming: {e}")

        # Wait for MCP server to be ready with health check
        server_url = f"http://localhost:{port}/mcp"
        logger.debug(f"[DEBUG] {server_url}: Waiting for server ready")

        server_ready = self.wait_for_server_ready(server_url, container_name)
        logger.debug(f"[DEBUG] {server_url}: Server is ready")

        # After server is ready, block internet using iptables
        if restrict_network:
            logger.debug(f"[DEBUG] Blocking internet for {container_name}")
            console.print("[yellow]üîí Configuring network isolation...[/yellow]")

            # Get container IP and gateway
            result = subprocess.run(
                [
                    "docker",
                    "inspect",
                    container_name,
                    "--format",
                    "{{.NetworkSettings.IPAddress}},{{.NetworkSettings.Gateway}}",
                ],
                capture_output=True,
                text=True,
            )
            ip_info = result.stdout.strip().split(",")

            if len(ip_info) == 2:
                container_ip = ip_info[0]
                gateway_ip = ip_info[1]

                # Use iptables to block internet while allowing host communication
                iptables_script = f"""
apk add iptables >/dev/null 2>&1
# Insert in reverse order (last rule first, first rule last)
# Drop everything else (public internet) - insert last
iptables -I DOCKER-USER 1 -s {container_ip} -j DROP
# Allow to private networks (RFC1918)
iptables -I DOCKER-USER 1 -s {container_ip} -d 192.168.0.0/16 -j ACCEPT
iptables -I DOCKER-USER 1 -s {container_ip} -d 172.16.0.0/12 -j ACCEPT
iptables -I DOCKER-USER 1 -s {container_ip} -d 10.0.0.0/8 -j ACCEPT
# Allow DNS
iptables -I DOCKER-USER 1 -s {container_ip} -p tcp --dport 53 -j ACCEPT
iptables -I DOCKER-USER 1 -s {container_ip} -p udp --dport 53 -j ACCEPT
# Allow all traffic to Docker gateway (host) - insert first
iptables -I DOCKER-USER 1 -s {container_ip} -d {gateway_ip} -j ACCEPT
"""

                result = subprocess.run(
                    [
                        "docker",
                        "run",
                        "--rm",
                        "--privileged",
                        "--net=host",
                        "alpine",
                        "sh",
                        "-c",
                        iptables_script,
                    ],
                    capture_output=True,
                    text=True,
                )

                if result.returncode == 0:
                    logger.debug(
                        f"[DEBUG] Successfully blocked internet for {container_ip}"
                    )
                    console.print(
                        f"[green]‚úì Network isolation enabled for {container_ip} - internet blocked[/green]"
                    )
                else:
                    logger.warning(f"Failed to configure iptables: {result.stderr}")
                    console.print(
                        f"[yellow]‚ö† Failed to configure iptables: {result.stderr.strip()}[/yellow]"
                    )
            else:
                logger.warning("Could not get container network info")
                console.print(
                    "[yellow]‚ö† Could not get container network info - internet not blocked[/yellow]"
                )

        if not server_ready:
            # Clean up the container if server never became ready
            self.stop_container(container_name)
            raise RuntimeError(
                f"{server_url}: MCP server failed to start within 10 minutes"
            )

        return container_name

    def stop_container(self, container_name: str):
        """Stop a Docker container (without removing it)."""
        try:
            subprocess.run(
                ["docker", "stop", container_name], capture_output=True, text=True
            )
        except Exception as e:
            console.print(
                f"[yellow]Warning: Failed to stop container {container_name}: {e}[/yellow]"
            )

    def remove_container(self, container_name: str):
        """Remove a Docker container and clean up iptables rules."""
        try:
            # Get container IP before removing it
            result = subprocess.run(
                [
                    "docker",
                    "inspect",
                    container_name,
                    "--format",
                    "{{.NetworkSettings.IPAddress}}",
                ],
                capture_output=True,
                text=True,
            )
            container_ip = result.stdout.strip()

            # Remove iptables rules for this container
            if container_ip:
                cleanup_script = f"""
apk add iptables >/dev/null 2>&1
# Keep deleting rules until none match (handles multiple rules)
while iptables -L DOCKER-USER -n --line-numbers | grep -q {container_ip}; do
    line=$(iptables -L DOCKER-USER -n --line-numbers | grep {container_ip} | head -1 | awk '{{print $1}}')
    iptables -D DOCKER-USER $line
done
"""
                subprocess.run(
                    [
                        "docker",
                        "run",
                        "--rm",
                        "--privileged",
                        "--net=host",
                        "alpine",
                        "sh",
                        "-c",
                        cleanup_script,
                    ],
                    capture_output=True,
                    text=True,
                )

            # Remove the container
            subprocess.run(
                ["docker", "rm", "-f", container_name], capture_output=True, text=True
            )
        except Exception as e:
            console.print(
                f"[yellow]Warning: Failed to remove container {container_name}: {e}[/yellow]"
            )
        finally:
            self.running_containers.discard(container_name)

    def cleanup_all(self):
        """Remove all running containers (assumes they're already stopped)."""
        containers_to_cleanup = list(self.running_containers)
        for container in containers_to_cleanup:
            self.remove_container(container)


class EvalRunner:
    def __init__(
        self,
        model: str = "starburst",
        max_tokens: int = 1000,
        max_concurrency: int = 3,
        transcript_dir: str = "eval_transcripts",
        debug_mode: bool = False,
        max_turns: int = 50,
        # New streaming parameters
        stream_results: bool = False,
        apex_server_url: Optional[str] = None,
        apex_api_key: Optional[str] = None,
        # Validation parameters
        skip_validation: bool = False,
        # Task version tracking
        task_versions: Optional[Dict[str, Any]] = None,
        # Existing evaluation
        evaluation_id: Optional[str] = None,
        # Container cleanup
        cleanup_existing: bool = False,
        # Computer use
        enable_computer_use: bool = False,
        # Agent type
        agent_type: Optional[str] = None,
        # Updated tasks (tasks that were downloaded/updated)
        updated_tasks: Optional[List[str]] = None,
        # CPU allocation
        cpus: float = 4.0,
        # Container cleanup after evaluation
        no_cleanup: bool = False,
        # Task format
        task_format: str = "apex",
    ):
        self.model = model
        self.max_tokens = max_tokens
        self.max_concurrency = max_concurrency
        self.transcript_dir = Path(transcript_dir)
        self.transcript_dir.mkdir(exist_ok=True)
        # Create server logs directory
        self.server_logs_dir = self.transcript_dir / "server_logs"
        self.server_logs_dir.mkdir(exist_ok=True)
        self.debug_mode = debug_mode
        self.max_turns = max_turns
        self.docker_manager = DockerManager()
        self.semaphore = asyncio.Semaphore(max_concurrency)

        # New streaming configuration
        self.stream_results = stream_results
        self.apex_server_url = apex_server_url
        self.apex_api_key = apex_api_key
        self.evaluation_id = evaluation_id

        # Validation configuration
        self.skip_validation = skip_validation

        # Task version tracking
        self.task_versions = task_versions or {}

        # Container cleanup configuration
        self.cleanup_existing = cleanup_existing

        # Computer use configuration
        self.enable_computer_use = enable_computer_use

        # Agent type configuration - will be resolved after format adapter is initialized
        self._agent_type_override = agent_type

        # Updated tasks tracking
        self.updated_tasks = updated_tasks or []

        # CPU allocation configuration
        self.cpus = cpus

        # Container cleanup configuration
        self.no_cleanup = no_cleanup

        # Task format configuration
        self.task_format = task_format

        # Initialize format adapter
        from .task_formats import FormatRegistry

        self.format_adapter = FormatRegistry.get_adapter(task_format)

        # Resolve agent type - use override if specified, otherwise use format default
        if self._agent_type_override:
            self.agent_type = self._agent_type_override
        else:
            self.agent_type = self.format_adapter.get_default_agent()

        self.executor = ThreadPoolExecutor(max_workers=os.cpu_count() - 1)
        # Validate streaming configuration
        if self.stream_results:
            if not self.apex_server_url:
                raise ValueError("apex_server_url is required when stream_results=True")
            if not self.apex_api_key:
                raise ValueError("apex_api_key is required when stream_results=True")

    def discover_tasks(self, tasks_dir: Path = None) -> List[str]:
        """Discover all valid tasks in the tasks directory.

        Uses the format adapter to determine which tasks are valid
        based on the configured task format.
        """
        if tasks_dir is None:
            # Import here to avoid circular imports
            import os

            tasks_dir_str = os.getenv("APEX_TASK_DIR", "tasks")
            tasks_dir = Path(tasks_dir_str).resolve()

        if not tasks_dir.exists():
            console.print(
                f"[yellow]Tasks directory {tasks_dir} does not exist[/yellow]"
            )
            return []

        # Use format adapter to discover tasks
        valid_tasks = self.format_adapter.discover_tasks(tasks_dir)

        if self.debug_mode:
            console.print(
                f"[dim]Found {len(valid_tasks)} valid {self.task_format} format tasks[/dim]"
            )

        return valid_tasks

    async def run_single_evaluation(
        self,
        task: str,
        run: int,
        streamer: Optional[ResultStreamer] = None,
        evaluation_id: Optional[str] = None,
        image_name: Optional[str] = None,
    ) -> EvalResult:
        """Run a single evaluation for a task."""
        # For harbor format, delegate to harbor CLI
        if self.task_format == "harbor":
            return await self._run_harbor_evaluation(task, run, streamer, evaluation_id)

        # Create transcript file
        run_id = f"{task}-run{run}"
        transcript_file = self.transcript_dir / f"{run_id}.json"

        # Generate container name early so we can use it for log file naming
        timestamp = int(time.time())
        container_name_prefix = f"apex-arena-eval-{task}-{run}-{timestamp}"

        # Create server log file path using container name (will be mounted into container)
        server_log_file = self.server_logs_dir / f"{container_name_prefix}.log"

        # Entrypoint log file (captures full docker logs including entrypoint)
        entrypoint_log_file = (
            self.server_logs_dir / f"{container_name_prefix}-entrypoint.log"
        )

        # Container startup retry logic
        max_container_retries = 3
        container_name = None
        port = None
        start_time = time.time()

        for container_attempt in range(max_container_retries):
            try:
                try:
                    port = _allocate_port()
                    # Try to start container and wait for it to be ready
                    loop = asyncio.get_event_loop()
                    container_name = await loop.run_in_executor(
                        self.executor,
                        lambda: self.docker_manager.start_container(
                            task,
                            run,
                            port,
                            server_log_file,
                            image_name,
                            container_name_prefix,
                            self.cpus,
                        ),
                    )

                    # If we get here, container started successfully
                    console.print(
                        f"[green]‚úì Container started successfully: {container_name}[/green]"
                    )
                    break  # Success, exit container retry loop

                except Exception as container_error:
                    # Container startup failed
                    if container_attempt < max_container_retries - 1:
                        console.print(
                            f"[yellow]‚ö† Container startup failed (attempt {container_attempt + 1}/{max_container_retries}): {str(container_error)}[/yellow]"
                        )
                        console.print(
                            "[yellow]  Retrying with new container and port...[/yellow]"
                        )

                        # Clean up failed container if it exists
                        if container_name:
                            self.docker_manager.stop_container(container_name)
                            container_name = None

                        # Release current port
                        port = None

                        # Wait before retrying
                        await asyncio.sleep(2)
                        continue
                    else:
                        # Final attempt failed
                        console.print(
                            f"[red]‚úó Container startup failed after {max_container_retries} attempts: {str(container_error)}[/red]"
                        )
                        raise

            except Exception as e:
                # Clean up on any error
                if container_name:
                    self.docker_manager.stop_container(container_name)
                raise e

        # At this point, we should have a running container
        if not container_name or not port:
            raise RuntimeError("Failed to start container after all retries")

        try:
            # Get task version ID for this task
            task_version_id = self.task_versions.get(task)

            # Stream rollout start if streaming is enabled
            rollout_id = None
            if streamer and evaluation_id:
                try:
                    rollout_id = await streamer.stream_rollout(
                        evaluation_id=evaluation_id,
                        local_task_id=task,
                        run_number=run,
                        success=False,  # Will update to True on success
                        task_version_id=task_version_id,
                    )
                except Exception as e:
                    console.print(
                        f"[yellow]Warning: Failed to start rollout streaming: {type(e)}: {e}[/yellow]"
                    )

            server_url = f"http://localhost:{port}/mcp"

            client = LoggingApexArenaClient(
                server_url,
                self.model,
                self.max_tokens,
                run_id,
                transcript_file,
                self.max_turns,
                # Streaming parameters
                streamer=streamer,
                evaluation_id=evaluation_id,
                rollout_id=rollout_id,
                # Task version tracking
                task_version_id=task_version_id,
                # Computer use
                enable_computer_use=self.enable_computer_use,
                # Agent type
                agent_type=self.agent_type,
            )

            result = await client.run_problem(task)
            duration = time.time() - start_time

            # Stream final rollout result if streaming is enabled
            if streamer and evaluation_id:
                try:
                    extracted_score = extract_score_from_grade_result(
                        result["grade_result"]
                    )
                    await streamer.stream_rollout(
                        evaluation_id=evaluation_id,
                        local_task_id=task,
                        run_number=run,
                        success=True,
                        grade_result=result["grade_result"],
                        extracted_score=extracted_score,
                        task_version_id=task_version_id,
                    )
                except Exception as e:
                    console.print(
                        f"[yellow]Warning: Failed to stream final rollout: {e}[/yellow]"
                    )

            return EvalResult(
                task=task,
                run=run,
                grade_result=result["grade_result"],
                duration=duration,
                success=True,
                transcript_file=str(transcript_file),
                server_log_file=str(server_log_file),
                entrypoint_log_file=str(entrypoint_log_file)
                if entrypoint_log_file.exists()
                else None,
            )

        except Exception as e:
            duration = time.time() - start_time
            return EvalResult(
                task=task,
                run=run,
                grade_result=None,
                duration=duration,
                success=False,
                error=str(e),
                transcript_file=str(transcript_file)
                if transcript_file.exists()
                else None,
                server_log_file=str(server_log_file)
                if server_log_file.exists()
                else None,
                entrypoint_log_file=str(entrypoint_log_file)
                if entrypoint_log_file.exists()
                else None,
            )
        finally:
            # Always stop container to free resources immediately
            if container_name:
                self.docker_manager.stop_container(container_name)
                # Remove container if --no-cleanup is not set
                if not self.no_cleanup:
                    self.docker_manager.remove_container(container_name)

    def extract_score(self, grade_result: Any) -> float:
        """Extract numerical score from grade result."""
        try:
            if isinstance(grade_result, str):
                # Try to parse as JSON
                grade_data = json.loads(grade_result)
            else:
                grade_data = grade_result

            if isinstance(grade_data, dict):
                if "subscores" in grade_data and "weights" in grade_data:
                    # Calculate weighted score
                    subscores = grade_data["subscores"]
                    weights = grade_data["weights"]
                    total_score = sum(
                        subscores[key] * weights[key] for key in subscores.keys()
                    )
                    return total_score
                elif "score" in grade_data:
                    return float(grade_data["score"])

            # If it's already a number
            if isinstance(grade_data, (int, float)):
                return float(grade_data)

            return 0.0
        except Exception:
            return 0.0

    def aggregate_results(
        self, results: List[EvalResult]
    ) -> Dict[str, AggregatedResult]:
        """Aggregate results by task."""
        task_results = {}

        for task in set(r.task for r in results):
            task_evals = [r for r in results if r.task == task]
            successful_evals = [r for r in task_evals if r.success]

            if successful_evals:
                scores = [self.extract_score(r.grade_result) for r in successful_evals]
                mean_score = statistics.mean(scores)
                std_dev = statistics.stdev(scores) if len(scores) > 1 else 0.0
            else:
                scores = []
                mean_score = 0.0
                std_dev = 0.0

            # Calculate coefficient of variation (CV = std_dev / mean)
            # Measures relative variability of scores (which are in range 0-1)
            # Lower CV = more consistent performance across runs
            if mean_score > 0:
                coefficient_of_variation = std_dev / mean_score
            else:
                coefficient_of_variation = 0.0

            success_rate = len(successful_evals) / len(task_evals)

            task_results[task] = AggregatedResult(
                task=task,
                mean_score=mean_score,
                std_dev=std_dev,
                success_rate=success_rate,
                total_runs=len(task_evals),
                scores=scores,
                coefficient_of_variation=coefficient_of_variation,
            )

        return task_results

    async def validate_tasks_structure(self, tasks: List[str]) -> Dict[str, Any]:
        """Validate task folder structure before running evaluation."""
        import os

        from .utils import validate_task_folder_structure

        tasks_dir_str = os.getenv("APEX_TASK_DIR", "tasks")
        tasks_dir = Path(tasks_dir_str).resolve()

        structure_results = {}
        structure_issues_found = False

        console.print(
            f"[yellow]Validating task folder structures ({self.task_format} format)...[/yellow]"
        )

        for task in tasks:
            task_dir = tasks_dir / task
            is_valid, errors = validate_task_folder_structure(
                task_dir, self.task_format
            )

            structure_results[task] = {"valid": is_valid, "errors": errors}

            if is_valid:
                console.print(f"[green]‚úÖ {task}: folder structure valid[/green]")
            else:
                structure_issues_found = True
                console.print(f"[red]‚ùå {task}: folder structure invalid[/red]")
                for error in errors:
                    console.print(f"  [red]‚Ä¢ {error}[/red]")

        return {
            "structure_results": structure_results,
            "structure_issues_found": structure_issues_found,
        }

    async def validate_tasks_quality(self, tasks: List[str]) -> Dict[str, Any]:
        """Run quality checks on all tasks before running evaluation."""
        import os

        from .quality_checker import QualityChecker

        tasks_dir_str = os.getenv("APEX_TASK_DIR", "tasks")
        tasks_dir = Path(tasks_dir_str).resolve()

        validation_results = {}
        issues_found = False

        console.print("[yellow]Running quality checks on tasks...[/yellow]")

        for task in tasks:
            task_dir = tasks_dir / task
            if task_dir.exists() and task_dir.is_dir():
                quality_checker = None
                try:
                    quality_checker = QualityChecker(task_dir, "biggie")
                    result = await quality_checker.check()
                    validation_results[task] = result

                    # Check for errors or failed checks
                    has_errors = "error" in result
                    failed_checks = (
                        0
                        if has_errors
                        else sum(
                            1
                            for key in result
                            if key != "_metadata"
                            and isinstance(result[key], dict)
                            and result[key].get("outcome") == "fail"
                        )
                    )

                    # Always print the quality check report
                    console.print(
                        f"\n[bold yellow]üìã Quality Check Report for {task}:[/bold yellow]"
                    )

                    if has_errors:
                        issues_found = True
                        console.print(
                            f"  [red]‚ùå Quality check failed: {result.get('error', 'Unknown error')}[/red]"
                        )
                    else:
                        # Display categorized results using the format_results method
                        formatted_results = quality_checker.format_results(result)
                        # Split into lines and indent for display
                        for line in formatted_results.split("\n"):
                            if line.strip():
                                console.print(f"  {line}")

                    if failed_checks > 0:
                        issues_found = True
                        if failed_checks > 3:
                            console.print(
                                f"  [red]‚ùå {failed_checks} quality issues found - significant improvements needed[/red]"
                            )
                        else:
                            console.print(
                                f"  [yellow]‚ö†Ô∏è {failed_checks} quality issues found - consider addressing them[/yellow]"
                            )
                    else:
                        console.print(
                            f"  [green]‚úÖ All quality checks passed for {task}[/green]"
                        )

                except Exception as e:
                    validation_results[task] = {"error": str(e)}
                    console.print(
                        f"[red]‚úó Failed to run quality check on {task}: {e}[/red]"
                    )
                    issues_found = True
            else:
                validation_results[task] = {"error": "task directory not found"}
                console.print(f"[red]‚úó {task} directory not found[/red]")
                issues_found = True

        return {"validation_results": validation_results, "issues_found": issues_found}

    async def run_evaluation(
        self, tasks: List[str], runs: int, force_build: bool = False
    ) -> Dict[str, Any]:
        """Run full evaluation across tasks and runs."""
        console.print(
            f"[green]Starting evaluation: {len(tasks)} tasks √ó {runs} runs = {len(tasks) * runs} total evaluations[/green]"
        )
        console.print(f"[blue]Max concurrency: {self.max_concurrency}[/blue]")
        # For Harbor format, use the CLI-specified agent; for Apex, use internal agent types
        if self.task_format == "harbor":
            console.print(
                f"[cyan]Using Harbor agent: {self.agent_type or 'default'} for model: {self.model}[/cyan]"
            )
        else:
            # starburst and starburst-plus use TerminusV1Agent, all others use MCPAgent
            internal_agent = (
                "TerminusV1Agent"
                if self.model in ("starburst", "starburst-plus")
                else "MCPAgent"
            )
            console.print(
                f"[cyan]Using agent: {internal_agent} for model: {self.model}[/cyan]"
            )

        # Validate folder structure (can be skipped with --skip-validation)
        structure_summary = None
        if not self.skip_validation:
            structure_summary = await self.validate_tasks_structure(tasks)

            if structure_summary["structure_issues_found"]:
                console.print(
                    "\n[red]Task folder structure validation found issues.[/red]"
                )
                from rich.prompt import Confirm

                if not Confirm.ask(
                    "Do you want to continue with the evaluation despite structure issues?",
                    default=False,
                ):
                    console.print("[yellow]Evaluation cancelled.[/yellow]")
                    metadata = {
                        "timestamp": datetime.datetime.now(datetime.UTC).isoformat(),
                        "cancelled": True,
                    }
                    if structure_summary:
                        metadata["structure_results"] = structure_summary[
                            "structure_results"
                        ]
                    return {"metadata": metadata}
            else:
                console.print("[green]‚úì All task folder structures are valid![/green]")

        # Run quality checks (can be skipped with --skip-validation)
        # Quality checks are Apex-specific, skip for Harbor format
        if not self.skip_validation and self.task_format == "apex":
            validation_summary = await self.validate_tasks_quality(tasks)

            if validation_summary["issues_found"]:
                console.print("\n[red]Quality checks found issues with tasks.[/red]")
                from rich.prompt import Confirm

                if not Confirm.ask(
                    "Do you want to continue with the evaluation?", default=False
                ):
                    console.print("[yellow]Evaluation cancelled.[/yellow]")
                    metadata = {
                        "timestamp": datetime.datetime.now(datetime.UTC).isoformat(),
                        "cancelled": True,
                        "validation_results": validation_summary["validation_results"],
                    }
                    if structure_summary:
                        metadata["structure_results"] = structure_summary[
                            "structure_results"
                        ]
                    return {"metadata": metadata}
            else:
                console.print("[green]‚úì All quality checks passed![/green]")
        elif self.task_format == "harbor":
            console.print(
                "[dim]Quality checks skipped for harbor format (use 'harbor tasks check' for harbor-specific validation)[/dim]"
            )
        else:
            console.print(
                "[yellow]‚ö† Validation checks skipped (both quality and structure)[/yellow]"
            )

        # Initialize streaming if enabled
        streamer = None
        evaluation_id = self.evaluation_id  # Use existing evaluation_id if provided

        if self.stream_results:
            console.print(f"[cyan]Streaming results to: {self.apex_server_url}[/cyan]")
            try:
                streamer = ResultStreamer(self.apex_server_url, self.apex_api_key)

                # Only create new evaluation if evaluation_id not provided
                if not evaluation_id:
                    evaluation_id = await streamer.start_evaluation(
                        tasks=tasks,
                        runs_per_task=runs,
                        model=self.model,
                        max_tokens=self.max_tokens,
                        max_concurrency=self.max_concurrency,
                        enable_computer_use=self.enable_computer_use,
                        agent_type=self.agent_type,
                    )
                    console.print(
                        f"[green]Started remote evaluation: {evaluation_id}[/green]"
                    )
                else:
                    console.print(
                        f"[green]Using existing evaluation: {evaluation_id}[/green]"
                    )
            except Exception as e:
                console.print(f"[red]Failed to initialize streaming: {e}[/red]")
                console.print(
                    "[yellow]Continuing with local evaluation only...[/yellow]"
                )
                streamer = None
                evaluation_id = None

        # Check for existing apex-arena containers
        existing_containers_result = subprocess.run(
            [
                "docker",
                "ps",
                "-a",
                "--filter",
                "name=apex-arena",
                "--format",
                "{{.Names}}",
            ],
            capture_output=True,
            text=True,
        )
        existing_containers = [
            name.strip()
            for name in existing_containers_result.stdout.split("\n")
            if name.strip()
        ]

        if existing_containers:
            console.print(
                f"[yellow]Found existing apex-arena containers: {', '.join(existing_containers)}[/yellow]"
            )

            # Determine cleanup behavior based on flag
            should_cleanup = self.cleanup_existing

            if should_cleanup:
                console.print("[yellow]Cleaning up existing containers...[/yellow]")
                result = subprocess.run(
                    ["docker", "rm", "-f"] + existing_containers,
                    capture_output=True,
                    text=True,
                )
                if result.stdout:
                    console.print(
                        f"[yellow]Removed containers: {result.stdout}[/yellow]"
                    )
            else:
                console.print("[blue]Keeping existing containers.[/blue]")

        console.print("[yellow]Building task images...[/yellow]")
        with tqdm(total=len(tasks)) as pbar:
            futs = []
            for task in tasks:
                # Force rebuild task if it was updated OR if force_build flag is set
                # Only rebuild base if user explicitly requested --force-build
                should_force_task = force_build or (task in self.updated_tasks)
                fut = self.executor.submit(
                    build_docker_image,
                    task,
                    force_base=force_build,
                    force_task=should_force_task,
                )
                futs.append(fut)

            for _ in concurrent.futures.as_completed(futs):
                pbar.update(1)

        # Create all evaluation tasks
        tasks_to_run = []

        async def run_with_semaphore(task, run, image_name):
            async with self.semaphore:
                result = await self.run_single_evaluation(
                    task, run, streamer, evaluation_id, image_name
                )
                return result

        for task in tasks:
            image_name = f"apex_arena:{task}"
            for run in range(1, runs + 1):
                tasks_to_run.append(run_with_semaphore(task, run, image_name))

        # Run all evaluations with progress tracking
        results = []

        # Set up progress manager based on debug mode
        global eval_progress
        eval_progress = ProgressManager(use_live=not self.debug_mode)

        eval_progress.start_progress(len(tasks_to_run))

        try:
            # Start all tasks and process them as they complete
            for completed_task in asyncio.as_completed(tasks_to_run):
                try:
                    result = await completed_task
                    results.append(result)

                    if result.success:
                        console.print(
                            f"[green]‚úì COMPLETED[/green] {result.task} run {result.run}: {result.duration:.1f}s [dim]({result.transcript_file})[/dim]"
                        )
                        console.print(
                            f"  [dim][yellow]logs: {result.entrypoint_log_file}[/yellow][/dim]"
                        )
                    else:
                        console.print(
                            f"[red]‚úó FAILED[/red] {result.task} run {result.run}: {result.error}"
                        )
                        console.print(
                            f"  [dim][yellow]logs: {result.entrypoint_log_file}[/yellow][/dim]"
                        )

                except Exception as e:
                    console.print(
                        f"[red]Task failed with exception: {escape(str(e))}[/red]"
                    )

                eval_progress.update_progress()
        finally:
            eval_progress.stop_progress()

        # Aggregate results
        aggregated = self.aggregate_results(results)

        # Complete streaming if enabled
        if streamer and evaluation_id:
            try:
                success_count = sum(1 for r in results if r.success)
                if success_count == len(results):
                    await streamer.complete_evaluation(evaluation_id, "completed")
                else:
                    await streamer.complete_evaluation(
                        evaluation_id, "completed"
                    )  # Still completed even with some failures
                console.print(
                    f"[green]Completed remote evaluation: {evaluation_id}[/green]"
                )
            except Exception as e:
                console.print(
                    f"[yellow]Warning: Failed to complete remote evaluation: {e}[/yellow]"
                )

        # Create final results structure
        eval_results = {
            "metadata": {
                "timestamp": datetime.datetime.now(datetime.UTC).isoformat(),
                "model": self.model,
                # starburst and starburst-plus use TerminusV1Agent, all others use MCPAgent
                "agent_type": "TerminusV1Agent"
                if self.model in ("starburst", "starburst-plus")
                else "MCPAgent",
                "max_tokens": self.max_tokens,
                "max_concurrency": self.max_concurrency,
                "tasks": tasks,
                "runs_per_task": runs,
                "total_evaluations": len(results),
                "evaluation_id": evaluation_id,  # Include remote evaluation ID if available
            },
            "individual_results": [asdict(r) for r in results],
            "aggregated_results": {k: asdict(v) for k, v in aggregated.items()},
        }

        return eval_results

    async def _run_harbor_evaluation(
        self,
        task: str,
        run: int,
        streamer: Optional[ResultStreamer] = None,
        evaluation_id: Optional[str] = None,
    ) -> EvalResult:
        """Run a single evaluation using Harbor CLI.

        For harbor format tasks, we delegate execution entirely to the harbor CLI.
        Harbor handles Docker, agent loop, and grading internally.
        """
        import os

        start_time = time.time()
        run_id = f"{task}-run{run}"

        # Get tasks directory
        tasks_dir_str = os.getenv("APEX_TASK_DIR", "tasks")
        tasks_dir = Path(tasks_dir_str).resolve()
        task_dir = tasks_dir / task

        # Load task info to get timeout
        task_info = self.format_adapter.load_task(task, tasks_dir)
        timeout = self.format_adapter.get_timeout(task_info)

        console.print(
            f"[blue]Running harbor evaluation for {task} (run {run})...[/blue]"
        )

        # Get task version ID for this task (if available)
        task_version_id = self.task_versions.get(task)

        # Stream rollout start if streaming is enabled
        rollout_id = None
        if streamer and evaluation_id:
            try:
                rollout_id = await streamer.stream_rollout(
                    evaluation_id=evaluation_id,
                    local_task_id=task,
                    run_number=run,
                    success=False,
                    task_version_id=task_version_id,
                    format="harbor",
                )
            except Exception as e:
                console.print(
                    f"[yellow]Warning: Failed to start rollout streaming: {e}[/yellow]"
                )

        try:
            # Use format adapter to run and grade
            # The harbor adapter handles calling `harbor run` and reading reward files
            # Pass evaluation_id and rollout_id for trajectory upload
            result = await self.format_adapter.run_and_grade(
                task_id=task,
                task_dir=task_dir,
                model=self.model,
                agent=self.agent_type or "default",
                timeout=timeout,
                evaluation_id=evaluation_id,
                rollout_id=rollout_id,
            )

            duration = time.time() - start_time

            # Convert GradingResultUnified to grade_result dict
            grade_result = result.to_dict()

            # Stream final rollout result if streaming is enabled
            if streamer and evaluation_id:
                try:
                    extracted_score = result.score
                    await streamer.stream_rollout(
                        evaluation_id=evaluation_id,
                        local_task_id=task,
                        run_number=run,
                        success=True,
                        grade_result=grade_result,
                        extracted_score=extracted_score,
                        task_version_id=task_version_id,
                        format="harbor",
                    )
                except Exception as e:
                    console.print(
                        f"[yellow]Warning: Failed to stream final rollout: {e}[/yellow]"
                    )

            console.print(
                f"[green]‚úì Harbor evaluation completed for {task} run {run}: score={result.score:.3f}[/green]"
            )

            return EvalResult(
                task=task,
                run=run,
                grade_result=grade_result,
                duration=duration,
                success=True,
                error=None,
                transcript_file=None,  # Harbor handles its own transcripts
                server_log_file=None,
                entrypoint_log_file=None,
            )

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)

            console.print(
                f"[red]‚úó Harbor evaluation failed for {task} run {run}: {error_msg}[/red]"
            )

            # Stream failure if streaming is enabled
            if streamer and evaluation_id and rollout_id:
                try:
                    await streamer.stream_rollout(
                        evaluation_id=evaluation_id,
                        local_task_id=task,
                        run_number=run,
                        success=False,
                        grade_result={"error": error_msg},
                        task_version_id=task_version_id,
                        format="harbor",
                    )
                except Exception as stream_error:
                    console.print(
                        f"[yellow]Warning: Failed to stream error: {stream_error}[/yellow]"
                    )

            return EvalResult(
                task=task,
                run=run,
                grade_result=None,
                duration=duration,
                success=False,
                error=error_msg,
                transcript_file=None,
                server_log_file=None,
                entrypoint_log_file=None,
            )

    async def cleanup(self):
        """Clean up resources."""
        if not self.no_cleanup:
            console.print("[yellow]Cleaning up containers...[/yellow]")
            self.docker_manager.cleanup_all()
        else:
            console.print(
                "[cyan]Skipping container cleanup (--no-cleanup flag set)[/cyan]"
            )
            console.print(
                f"[dim]Running containers: {', '.join(self.docker_manager.running_containers)}[/dim]"
            )
        self.executor.shutdown()


@contextlib.asynccontextmanager
async def eval_runner_context(*args, **kwargs):
    """Context manager for EvalRunner with automatic cleanup."""
    runner = EvalRunner(*args, **kwargs)
    try:
        yield runner
    finally:
        await runner.cleanup()
