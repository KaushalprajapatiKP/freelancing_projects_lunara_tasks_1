"""
Quality checker for Apex Arena tasks, based on terminal-bench quality checker.
Validates tasks against quality standards without using pydantic response models.
"""

import asyncio
import json
import os
from pathlib import Path
from typing import Any, Dict, Optional

from .llm import LLMClient


class QualityChecker:
    """Quality checker for Apex Arena tasks."""

    def __init__(
        self,
        task_dir: Path,
        model_name: str = "biggie-plus",
        grader_relative_path: str = "grader.py",
        dockerfile_relative_path: str = "Dockerfile",
        task_yaml_relative_path: str = "task.yaml",
    ):
        self.task_dir = task_dir
        self.grader_path = task_dir / grader_relative_path
        self.dockerfile_path = task_dir / dockerfile_relative_path
        self.task_yaml_path = task_dir / task_yaml_relative_path
        self.model_name = model_name
        self.proxy_client = None

    def _get_internet_context(self) -> Dict[str, str]:
        """Get environment context strings based on internet availability."""
        # Check if internet should be restricted (default: true)
        no_internet = os.getenv("NO_INTERNET", "true").lower()
        has_internet = no_internet in ["false", "f", "0"]
        
        if has_internet:
            return {
                "access_statement": "The task container HAS internet access - agents can install packages and access external resources after container starts",
                "cheating_constraints": "This means agents CAN: install additional packages, access external resources, and potentially look up solutions online",
                "anti_cheating_note": "Remember: agents have internet access and can install packages, but grader.py is still inaccessible",
                "cheating_risk_note": "given internet access and ability to install packages, grader.py being inaccessible, and access to external resources",
            }
        else:
            return {
                "access_statement": "The task container has NO internet access - agents cannot install packages after container starts",
                "cheating_constraints": "This means agents CANNOT cheat by: installing missing packages, reading grader.py, or accessing external resources",
                "anti_cheating_note": "Remember: no internet access, grader.py is inaccessible, agents can't install packages",
                "cheating_risk_note": "given no internet access, grader.py being inaccessible, and no package installations allowed",
            }

    async def _get_proxy_client(self) -> LLMClient:
        """Get or create proxy client."""
        if self.proxy_client is None:
            self.proxy_client = LLMClient()
        return self.proxy_client

    async def _cleanup_proxy_client(self):
        """Cleanup proxy client session."""
        if self.proxy_client is not None:
            try:
                await self.proxy_client.close()
            except Exception:
                pass  # Ignore cleanup errors
            finally:
                self.proxy_client = None

    def _load_file_content(self, file_path: Path) -> Optional[str]:
        """Load file content if it exists."""
        if not file_path.exists():
            return None
        try:
            return file_path.read_text(encoding="utf-8")
        except Exception:
            return None

    def _get_directory_structure(self, path: Path, prefix: str = "", max_depth: int = 10, current_depth: int = 0) -> str:
        """Generate a tree-like directory structure string."""
        if current_depth >= max_depth:
            return ""
        
        lines = []
        try:
            # Get all items and sort them (directories first, then files)
            items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
            
            for i, item in enumerate(items):
                is_last = i == len(items) - 1
                current_prefix = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
                next_prefix = "    " if is_last else "‚îÇ   "
                
                if item.is_dir():
                    lines.append(f"{prefix}{current_prefix}{item.name}/")
                    # Recursively get subdirectory structure
                    subdir_structure = self._get_directory_structure(
                        item, 
                        prefix + next_prefix, 
                        max_depth, 
                        current_depth + 1
                    )
                    if subdir_structure:
                        lines.append(subdir_structure)
                else:
                    # Add file size info
                    try:
                        size = item.stat().st_size
                        size_str = self._format_size(size)
                        lines.append(f"{prefix}{current_prefix}{item.name} ({size_str})")
                    except:
                        lines.append(f"{prefix}{current_prefix}{item.name}")
        except PermissionError:
            pass
        
        return "\n".join(lines)
    
    def _format_size(self, size: int) -> str:
        """Format file size in human-readable format."""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.1f}{unit}"
            size /= 1024.0
        return f"{size:.1f}TB"
    
    def _load_data_folder_files(self) -> Dict[str, str]:
        """Recursively load .md, .txt, and .yaml "documentation" files from data folder."""
        data_folder = self.task_dir / "data"
        loaded_files = {}
        
        if not data_folder.exists() or not data_folder.is_dir():
            return loaded_files
        
        # Recursively find files with specific extensions
        extensions = {'.md', '.txt', '.yaml', '.yml'}
        
        for file_path in data_folder.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in extensions:
                try:
                    content = file_path.read_text(encoding="utf-8")
                    # Store with relative path from data folder
                    relative_path = file_path.relative_to(data_folder)
                    loaded_files[str(relative_path)] = content
                except Exception as e:
                    # Store error message if file can't be read
                    relative_path = file_path.relative_to(data_folder)
                    loaded_files[str(relative_path)] = f"[Error reading file: {e}]"
        
        return loaded_files

    def _create_quality_check_prompt(
        self,
        task_yaml: str,
        grader_content: str,
        dockerfile: str,
        solution_content: Optional[str] = None,
        directory_structure: Optional[str] = None,
        data_folder_files: Optional[Dict[str, str]] = None,
    ) -> str:
        """Create the quality check prompt for the LLM."""

        solution_section = ""
        solution_exists = solution_content is not None
        if solution_content:
            solution_section = f"""
<solution.sh>
{solution_content}
</solution.sh>
"""
        else:
            solution_section = "\n<solution.sh>\nNOT FOUND - solution.sh file does not exist\n</solution.sh>\n"
        
        # Add directory structure section
        directory_section = ""
        if directory_structure:
            directory_section = f"""
<task_directory_structure>
{directory_structure}
</task_directory_structure>
"""
        
        # Add data folder files section
        data_files_section = ""
        if data_folder_files:
            data_files_parts = []
            for rel_path, content in data_folder_files.items():
                data_files_parts.append(f"""<data/{rel_path}>
{content}
</data/{rel_path}>""")
            
            if data_files_parts:
                data_files_section = "\n".join(data_files_parts) + "\n"

        # Include base Dockerfile for complete context
        base_dockerfile = """FROM docker.io/ubuntu:22.04

RUN groupadd -g 1000 model && \\
    useradd -m -u 1000 -g 1000 model && \\
    gpasswd -d model root || true

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends ca-certificates && apt-get clean

RUN apt-get update && apt-get install -y software-properties-common curl sudo

RUN add-apt-repository ppa:deadsnakes/ppa

RUN apt-get update && apt-get install -y \\
    curl \\
    && add-apt-repository ppa:deadsnakes/ppa \\
    && apt-get update \\
    && apt-get install -y \\
    python3.11 \\
    python3.11-venv \\
    python3.11-dev \\
    python3-pip \\
    wget \\
    && apt-get clean

RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 \\
    && update-alternatives --set python3 /usr/bin/python3.11 \\
    && ln -s /usr/bin/python3 /usr/bin/python

RUN python -m pip install --upgrade pip
RUN python -m pip install uv

WORKDIR /mcp_server
RUN chmod -R 0700 /mcp_server
RUN mkdir /tests && chmod -R 0700 /tests
RUN mkdir /test && chmod -R 0700 /test
RUN mkdir /grade && chmod -R 0700 /grade

RUN mkdir -p apex_arena
COPY . apex_arena
COPY pyproject.toml .

RUN uv pip install --system -e .

RUN uv pip install --system pytest
ENV WORKDIR=/workdir

RUN mkdir -p ${WORKDIR}

RUN chown -R model:model ${WORKDIR}
# Set root password - model user can only access root via su with this password
RUN echo 'root:mypassword' | chpasswd

# Ensure model user is NOT in sudoers (remove any existing sudo access)
RUN sed -i '/^model/d' /etc/sudoers 2>/dev/null || true

WORKDIR ${WORKDIR}"""

        # Get internet context based on environment configuration
        internet_ctx = self._get_internet_context()

        v0_prompt = f"""You are a quality checker for terminal bench style tasks. Analyze the following task components and return a JSON response with quality assessments.

IMPORTANT CONTAINER ENVIRONMENT CONTEXT:
- {internet_ctx['access_statement']}
- The grader.py is stored in /mcp_server and is INACCESSIBLE to the agent (different Linux user/permissions)
- Agents can only use tools provided by the MCP server and files in their working directory
- Agents CANNOT access /tests or /test directories, so any copy commands in Dockerfile that copy to these directories are safe from cheating.
- {internet_ctx['cheating_constraints']}

PAY SPECIAL ATTENTION TO:
1. ARBITRARY THRESHOLDS: Look for hardcoded pass/fail thresholds without clear justification (e.g., score > 0.7, performance > 100x)
   IMPORTANT NOTE: If the grader checks all test casses and returns binary scoring based on that i.e if all cases passed give 1.0 score, this is NOT arbitrary
2. REWARD HACKING: Check if agents could game the system through:
   - Hardcoding expected outputs based on test patterns
   - Modifying data files to make tests pass. 
   - Unless the task is explicitly designed to ask the agent to make changes to certain files within data folder, the grader.py should not be referring to those files in the data folder as these files are agent modifiable. Instead, the grader should be referring to a copy of the files in the /tests directory (protected from agent modification) OR the task should track changes to the files in the data folder using hashing/checksumming.
   - Using shortcuts that bypass the actual task requirements
3. NON-DETERMINISTIC BEHAVIOR: Check if grader handles randomness, file ordering, timing dependencies properly
4. SOLUTION FILE: solution.sh must exist and be executable for reference implementation

<task.yaml>
{task_yaml}
</task.yaml>

<grader.py>
{grader_content}
</grader.py>

<base_dockerfile>
This is the base Dockerfile that all tasks inherit from:
{base_dockerfile}
</base_dockerfile>

<task_dockerfile>
This is the task-specific Dockerfile:
{dockerfile}
</task_dockerfile>

<task_solution>
{solution_section}
</task_solution>

<task_directory_structure>
{directory_section}
</task_directory_structure>

<task_data_folder_documentation_files>
{data_files_section}
</task_data_folder_documentation_files>

Evaluate the task against these quality criteria and respond with ONLY valid JSON in this exact format:

{{
  "arbitrary_thresholds": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Are there hardcoded pass/fail thresholds without clear justification? Look for things like score > 0.7, len(result) >= 50, performance > 100x without explanation"
  }},
  "reward_hacking_vulnerabilities": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Could agents game the system by hardcoding outputs, modifying data files, or using shortcuts that bypass actual task requirements? Consider container isolation."
  }},
  "non_deterministic_behavior": {{
    "outcome": "pass|fail|not_applicable", 
    "explanation": "Does the grader properly handle randomness, file ordering, timing dependencies, or other sources of non-determinism?"
  }},
  "solution_file_quality": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Is the solution.sh a human-written complete reference implementation of the task? It should NOT have any shortcuts/hacks or unjustified hard-coded values not mentioned in the task.yaml or task_data_folder_documentation_files."
  }},
  "behavior_in_task_documentation": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Whether all behavior checked in the test cases is described in either in the task.yaml or the task_data_folder_documentation_files."
  }},
  "task_description_fluff": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Whether all the information in the task.yaml and the task_data_folder_documentation_files is relevant and actually needed to complete the task."
  }},
  "behavior_in_tests": {{
    "outcome": "pass|fail|not_applicable", 
    "explanation": "Whether all behavior described in the task description and task_data_folder_documentation_files is checked in the grader"
  }},
  "test_quality": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Whether the tests are robust and not brittle. Use of regex parsing and unjustified hard-coded values make tests brittle and should be flagged."
  }},
  "informative_test_docstrings": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Whether test cases have informative docstrings that describe which behavior they check"
  }},
  "anti_cheating_measures": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Is it hard for the agent to cheat on the task? {internet_ctx['anti_cheating_note']}"
  }},
  "structured_data_schema": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "If the agent produces structured data (e.g. it is tasked with building an API) whether the exact schema is described in the task.yaml or a separate file or in the task_data_folder_documentation_files"
  }},
  "pinned_dependencies": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "If the task uses external dependencies (e.g. Docker images, pip packages, etc.) whether their versions are pinned (versions should be pinned using ==) to ensure reproducibility"
  }},
  "typos": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Whether there are any typos. Look very closely at file and variable names, because these can be hard to catch"
  }},
  "task_clarity": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Is the task clearly described with unambiguous requirements and examples? Is the task premise cohesive and does it make sense? Are the task_data_folder_documentation_files (if it exists) helpful in understanding the task?"
  }},
  "task_premise_cohesiveness": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Does the task focus on a single coherent problem? Tasks that require the agent to perform multiple unrelated tasks is grounds for FAIL."
  }},
  "task_description_grader_references": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "The task description and task_data_folder_documentation_files should not refer to the grader or the evaluation requirements in any way."
  }},
  "grader_robustness": {{
    "outcome": "pass|fail|not_applicable", 
    "explanation": "Does the grader handle edge cases and variations in correct solutions appropriately?"
  }},
  "difficulty_alignment": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Is the stated difficulty level in the task.yaml metadata aligned with the actual complexity of the task?"
  }},
  "file_folder_naming_conventions": {{
    "outcome": "pass|fail|not_applicable",
    "explanation": "Whether all file and folder names are alphanumeric with dashes and underscores allowed with no spaces. Any other characters in the file and folder names is grounds for FAIL."
  }}
}}

JSON response:"""
        v1_prompt = f"""You are a quality checker for programming tasks. Analyze the following task components and return a JSON response with quality assessments.

IMPORTANT CONTAINER ENVIRONMENT CONTEXT:
- {internet_ctx['access_statement']}. All dependencies required for the task for both running and testing will be provided in the task_dockerfile.

{task_yaml}
</task.yaml>

<grader.py>
{grader_content}
</grader.py>

<solution.sh>
{solution_content}
</solution.sh>

<task_dockerfile>
This is the Dockerfile:
{dockerfile}
</task_dockerfile>

<task_directory_structure>
{directory_section}
</task_directory_structure>

<task_data_folder_documentation_files>
{data_files_section}
</task_data_folder_documentation_files>

I want you to find as many mistakes as possible in the task.yaml, task_data_folder_documentation_files, solution.sh, grader.py, and the task_dockerfile.

Mistakes can be classified as:

-	Solution Quality: Verify that solution.sh is a human-written complete reference implementation without shortcuts/hacks or unjustified hard-coded values not mentioned in task.yaml or task_data_folder_documentation_files.
-	Requirements Compliance: Identify any requirements specified in task.yaml or task_data_folder_documentation_files that are missing or not implemented in solution.sh.
-	Traceability Analysis: Determine whether solution.sh can be derived strictly from the information in task.yaml and task_data_folder_documentation_files. Flag ambiguities or evidence of workarounds that are not directly supported by the specification.
-	Task Documentation Coverage: Whether all behavior checked in the test cases is described either in task.yaml or task_data_folder_documentation_files.
-	Documentation Relevance: Whether all information in task.yaml and task_data_folder_documentation_files is relevant and actually needed to complete the task (no fluff).
-	Grader Coverage: Compare task.yaml and task_data_folder_documentation_files requirements against what grader.py tests. Report any requirements that are not verified by the grader.
-	Test Quality: Whether tests are robust and not brittle. Flag use of regex parsing and unjustified hard-coded values that make tests brittle.
-	Test Documentation: Check whether test cases include informative docstrings describing their intended behavior.
-	Grader Quality Audit: Provide a detailed evaluation of whether grader.py tests adequately enforce the requirements in task.yaml and task_data_folder_documentation_files, referencing both grader.py and solution.sh.
-	Threshold Justification: Identify hardcoded thresholds (e.g., score > 0.7, len(result) >= 50, performance > 100x) and note whether they lack clear justification.
-	Gaming Vulnerabilities: Assess whether the task setup could be exploited (e.g., hardcoded outputs, modifying data files and grader using those data files, or bypassing requirements) given container isolation constraints.
-	Non-determinism Handling: Evaluate how grader.py deals with randomness, file ordering, timing dependencies, or other non-deterministic factors.
-	Cheating Risk Assessment: Analyze ways an agent could bypass requirements ({internet_ctx['cheating_risk_note']}).
-	Schema Documentation: If the agent produces structured data (e.g., building an API), verify the exact schema is described in task.yaml or task_data_folder_documentation_files.
-	Dependency Reproducibility: Identify whether all dependencies have pinned versions (using ==) to ensure reproducibility.
-	Error/Typo Scan: Detect typographical errors in task.yaml, task_data_folder_documentation_files, solution.sh, and grader.py. Pay close attention to file and variable names.
-	Task Clarity: Is the task clearly described with unambiguous requirements? Does the premise make sense? Are task_data_folder_documentation_files (if present) helpful?
-	Task Cohesiveness: Does the task focus on a single coherent problem? Tasks requiring multiple unrelated tasks should FAIL.
-	Grader References: The task description and task_data_folder_documentation_files should not refer to the grader or evaluation requirements in any way.
-	Naming Conventions: Whether all file and folder names are alphanumeric with dashes and underscores allowed, no spaces. Any other characters should FAIL.

Think as much as you want and understand the task before starting to point out the mistakes. For each type of mistake, first try to analyse the mistakes and then give a final status. Do not begin with the status.

Finally, tell me if these mistakes are serious enough to reject the task or not.

At the end of your response, return a final verdict in <final_verdict></final_verdict> tags
with either "PASS" or "FAIL".
"""
        return v0_prompt, v1_prompt

    async def check(self) -> Dict[str, Any]:
        """Run quality check on the task."""

        try:
            # Load all task files
            task_yaml = self._load_file_content(self.task_yaml_path)
            grader_content = self._load_file_content(self.grader_path)
            dockerfile = self._load_file_content(self.dockerfile_path)

            # Try to load solution.sh if it exists
            solution_path = self.task_dir / "solution.sh"
            solution_content = self._load_file_content(solution_path)
            
            # Get directory structure
            directory_structure = self._get_directory_structure(self.task_dir)
            
            # Load data folder files (.md, .txt, .yaml)
            data_folder_files = self._load_data_folder_files()

            if not task_yaml:
                result = {"error": "task.yaml not found or could not be read"}
                self._save_quality_report(result)
                return result
            if not grader_content:
                result = {"error": "grader.py not found or could not be read"}
                self._save_quality_report(result)
                return result
            if not dockerfile:
                result = {"error": "Dockerfile not found or could not be read"}
                self._save_quality_report(result)
                return result

            # Create the prompt
            v0_prompt, v1_prompt = self._create_quality_check_prompt(
                task_yaml, grader_content, dockerfile, solution_content,
                directory_structure, data_folder_files
            )

            try:
                # Get LLM response
                proxy = await self._get_proxy_client()
                v0_task = proxy.create_message(
                    model=self.model_name,
                    messages=[{"role": "user", "content": v0_prompt}],
                    max_tokens=8000,
                    tools=None,
                    temperature=0,
                )
                v1_task = proxy.create_message(
                    model=self.model_name,
                    messages=[{"role": "user", "content": v1_prompt}],
                    max_tokens=8000,
                    tools=None,
                    temperature=0,
                )

                (v0_response, _), (v1_response, _) = await asyncio.gather(
                    v0_task, v1_task
                )

                v1_response = v1_response["content"][0]["text"]

                # Extract content from all text blocks
                if "content" not in v0_response or not v0_response["content"]:
                    result = {"error": "Empty response from LLM"}
                    self._save_quality_report(result)
                    return result

                # Combine all text content blocks
                text_parts = []
                for content_block in v0_response["content"]:
                    if content_block.get("type") == "text":
                        text_parts.append(content_block.get("text", ""))

                content = "\n".join(text_parts).strip()
                if not content:
                    result = {"error": "No text content found in LLM response"}
                    self._save_quality_report(result)
                    return result

                # Try to parse JSON from various formats
                json_str = self._extract_json(content)
                if not json_str:
                    result = {
                        "error": "No valid JSON found in LLM response",
                        "raw_response": content,
                    }
                    self._save_quality_report(result)
                    return result

                try:
                    result = json.loads(json_str)

                    # Validate the structure
                    expected_keys = [
                        "arbitrary_thresholds",
                        "reward_hacking_vulnerabilities",
                        "non_deterministic_behavior",
                        "solution_file_quality",
                        "behavior_in_task_documentation",
                        "task_description_fluff",
                        "behavior_in_tests",
                        "test_quality",
                        "informative_test_docstrings",
                        "anti_cheating_measures",
                        "structured_data_schema",
                        "pinned_dependencies",
                        "typos",
                        "task_clarity",
                        "task_premise_cohesiveness",
                        "task_description_grader_references",
                        "grader_robustness",
                        "difficulty_alignment",
                        "file_folder_naming_conventions",
                    ]

                    missing_keys = [key for key in expected_keys if key not in result]
                    if missing_keys:
                        result = {
                            "error": f"Missing required keys: {missing_keys}",
                            "partial_result": result,
                        }
                        self._save_quality_report(result)
                        return result

                    # Add metadata
                    result["_metadata"] = {
                        "task_dir": str(self.task_dir),
                        "task_id": self.task_dir.name,
                        "model_used": self.model_name,
                        "files_analyzed": {
                            "task_yaml": bool(task_yaml),
                            "grader_py": bool(grader_content),
                            "dockerfile": bool(dockerfile),
                            "solution_sh": bool(solution_content),
                            "directory_structure": bool(directory_structure),
                            "data_folder_files_count": len(data_folder_files) if data_folder_files else 0,
                            "data_folder_files": list(data_folder_files.keys()) if data_folder_files else [],
                        },
                    }

                    final_verdict = (
                        v1_response.split("<final_verdict>")[1]
                        .split("</final_verdict>")[0]
                        .lower()
                    )
                    result["v1_feedback"] = {
                        "outcome": final_verdict,
                        "explanation": v1_response,
                    }

                    # Save the quality report to JSON file
                    self._save_quality_report(result)

                    return result

                except json.JSONDecodeError as e:
                    result = {
                        "error": f"JSON parsing failed: {e}",
                        "raw_json": json_str,
                    }
                    self._save_quality_report(result)
                    return result

            except Exception as e:
                result = {"error": f"LLM call failed: {e}"}
                self._save_quality_report(result)
                return result

        finally:
            # Always cleanup the proxy client session
            await self._cleanup_proxy_client()

    def _extract_json(self, content: str) -> Optional[str]:
        """Extract JSON from LLM response content."""
        import re

        # Method 1: Look for JSON in code blocks
        json_match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", content, re.DOTALL)
        if json_match:
            return json_match.group(1)

        # Method 2: Look for JSON object starting from first {
        json_match = re.search(r"(\{.*\})", content, re.DOTALL)
        if json_match:
            return json_match.group(1)

        # Method 3: Check if the whole response is JSON
        if content.startswith("{") and content.endswith("}"):
            return content

        return None

    def format_results(self, results: Dict[str, Any]) -> str:
        """Format quality check results for display."""
        if "error" in results:
            return f"‚ùå Quality check failed: {results['error']}"

        output = []
        output.append("üìã **Quality Check Results**\n")

        # Group results by category
        categories = {
            "Critical Issues": [
                "arbitrary_thresholds",
                "reward_hacking_vulnerabilities",
                "non_deterministic_behavior",
                "solution_file_quality",
            ],
            "Task Design & Documentation": [
                "behavior_in_task_documentation",
                "task_description_fluff",
                "behavior_in_tests",
                "task_clarity",
                "task_premise_cohesiveness",
                "task_description_grader_references",
                "difficulty_alignment",
            ],
            "Testing & Validation": [
                "test_quality",
                "informative_test_docstrings",
                "grader_robustness",
            ],
            "Security & Anti-Cheating": [
                "anti_cheating_measures",
            ],
            "Technical Implementation": [
                "structured_data_schema",
                "pinned_dependencies",
            ],
            "Quality Assurance": [
                "typos",
                "file_folder_naming_conventions",
            ],
            "V1 Feedback (May Contain False Positives) (Meant to be extra strict --aid for reviewers)": ["v1_feedback"],
        }

        for category, keys in categories.items():
            output.append(f"## {category}")
            for key in keys:
                if key in results:
                    check = results[key]
                    outcome = check.get("outcome", "unknown")
                    explanation = check.get("explanation", "No explanation provided")

                    if outcome == "pass":
                        icon = "‚úÖ"
                    elif outcome == "fail":
                        icon = "‚ùå"
                    else:
                        icon = "‚ö™"

                    # Format the key name for display
                    display_name = key.replace("_", " ").title()
                    output.append(f"{icon} **{display_name}**: {explanation}")
            output.append("")

        # Summary
        total_checks = sum(
            1
            for key in results
            if key != "_metadata" and isinstance(results[key], dict)
        )
        passed_checks = sum(
            1
            for key in results
            if key != "_metadata"
            and isinstance(results[key], dict)
            and results[key].get("outcome") == "pass"
        )
        failed_checks = sum(
            1
            for key in results
            if key != "_metadata"
            and isinstance(results[key], dict)
            and results[key].get("outcome") == "fail"
        )

        output.append("## Summary")
        output.append(f"Total checks: {total_checks}")
        output.append(f"‚úÖ Passed: {passed_checks}")
        output.append(f"‚ùå Failed: {failed_checks}")
        output.append(
            f"‚ö™ Not applicable: {total_checks - passed_checks - failed_checks}"
        )

        if failed_checks == 0:
            output.append("\nüéâ **All applicable quality checks passed!**")
        elif failed_checks <= 2:
            output.append(
                f"\n‚ö†Ô∏è **{failed_checks} quality issues found** - consider addressing them"
            )
        else:
            output.append(
                f"\nüö® **{failed_checks} quality issues found** - significant improvements needed"
            )

        return "\n".join(output)

    def _save_quality_report(self, results: Dict[str, Any]) -> None:
        """Save quality check report to validation_reports folder."""
        import datetime

        try:
            # Create validation_reports directory if it doesn't exist
            reports_dir = Path("validation_reports")
            reports_dir.mkdir(exist_ok=True)

            # Get task ID from directory name
            task_id = self.task_dir.name

            # Create report filename
            report_file = reports_dir / f"{task_id}_quality_report.json"

            # Calculate summary statistics
            if "error" not in results:
                total_checks = sum(
                    1
                    for key in results
                    if key != "_metadata" and isinstance(results[key], dict)
                )
                passed_checks = sum(
                    1
                    for key in results
                    if key != "_metadata"
                    and isinstance(results[key], dict)
                    and results[key].get("outcome") == "pass"
                )
                failed_checks = sum(
                    1
                    for key in results
                    if key != "_metadata"
                    and isinstance(results[key], dict)
                    and results[key].get("outcome") == "fail"
                )
                na_checks = total_checks - passed_checks - failed_checks

                summary = {
                    "total_checks": total_checks,
                    "passed_checks": passed_checks,
                    "failed_checks": failed_checks,
                    "na_checks": na_checks,
                    "has_errors": False,
                }
            else:
                summary = {
                    "total_checks": 0,
                    "passed_checks": 0,
                    "failed_checks": 0,
                    "na_checks": 0,
                    "has_errors": True,
                    "error_message": results.get("error", "Unknown error"),
                }

            # Create comprehensive report data
            report_data = {
                "task_id": task_id,
                "timestamp": datetime.datetime.now(datetime.UTC).isoformat(),
                "qualit_save_quality_report_check_results": results,
                "summary": summary,
                "components_analyzed": results.get("_metadata", {}).get(
                    "files_analyzed", {}
                ),
                "model_used": self.model_name,
            }

            # Write report to file (overwrite if exists)
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)

            print(f"‚úÖ Quality report saved: {report_file}")

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to save quality report for {self.task_dir.name}: {e}")
