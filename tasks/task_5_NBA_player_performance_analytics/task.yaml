prompt: |
  Build a machine learning system to predict NBA player performance buckets using game and player metadata.

  CHALLENGE OVERVIEW:

  Predict NBA player performance buckets (0-49) from game and player metadata. The target variable
  is created by discretizing performance metrics into ~50 equal-frequency classes, creating a
  balanced multi-class classification problem. You must build a model that learns performance
  patterns from available metadata without using the direct performance metric as a feature.

  PROBLEM:

  Predict the performance bucket (0-49) for each NBA player game as a multi-class
  classification task using all available game and player metadata EXCEPT the direct
  performance metric itself.

  The performance buckets are created by:
    - Taking actual performance metric (e.g., minutes_played, game_score, or PER)
    - Discretizing into ~50 equal-frequency classes using quantile-based binning
    - Creating balanced classes where each bucket represents ~2% of games

  GOALS:

  Train a model using the training dataset provided at:
    /workdir/data/train.csv

  Make predictions on the test dataset provided at:
    /tests/test.csv

  The solution should:

    - Create a script at `/workdir/solution.sh` that implements the full ML pipeline
    - Load and validate the dataset schema
    - Analyze the data to understand performance patterns
    - Engineer features from available columns
    - Handle the multi-class classification problem with 50 balanced classes
    - Train one or more machine learning models optimized for Macro F1-Score
    - Save the trained model(s) and preprocessing artifacts to `/workdir/outputs/model.pkl` (or `/workdir/model.pkl`)
    - Create an inference script at `/workdir/predict.py` that:
      * Accepts two command-line arguments: model_path (first arg) and test_csv_path (second arg)
      * Loads the saved model from model_path
      * Reads test data from test_csv_path
      * Applies the same feature engineering used during training
      * Generates predictions and saves to `/workdir/outputs/predictions.csv`
    - Tune hyperparameters to maximize cross-validation performance
    - Generate predictions for the test set and save to `/workdir/outputs/predictions.csv`

  CONSTRAINTS:

  Allowed libraries:
    - Python 3.x
    - NumPy
    - pandas
    - scikit-learn
    - scipy
    - joblib
    - lightgbm
    - xgboost
    - imbalanced-learn

  Disallowed:
    - Deep learning frameworks (TensorFlow, PyTorch, JAX)
    - External API calls or internet access at runtime
    - Using performance_metric as a feature (this is explicitly forbidden - it is effectively the target)
    - Pre-trained embeddings or external data sources
    - Reading or accessing ground truth labels during training or inference

  The entire pipeline must be executable via a single shell entrypoint.

  EVALUATION:

  Your model will be evaluated using Macro-averaged F1 score (Macro F1). This metric computes
  the F1 score for each of the 50 performance bucket classes independently and then averages them,
  ensuring the model performs well across all performance levels.

  **Performance Expectations:**
  
  This is a challenging multi-class classification problem with 50 balanced classes (each class ~2% of data).
  The balanced class distribution means each bucket represents approximately 2% of the data, requiring
  models that can learn subtle patterns across all performance levels without overfitting to specific classes.
  
  Achieving strong performance requires:
    - Comprehensive feature engineering leveraging all available metadata
    - Advanced ensemble methods (LightGBM, XGBoost with proper hyperparameter tuning)
    - Careful handling of high-cardinality categorical variables (player names, colleges, teams)
    - Robust cross-validation strategies to ensure generalization
    - Balanced performance across all 50 classes (avoiding models that excel on some classes while failing on others)
  
  Strong models demonstrate:
    - High overall Macro F1 score indicating good average performance
    - Consistent per-class F1 scores with no catastrophic failures on individual classes
    - Good coverage across the full range of performance buckets

  OUTPUT FORMAT:

  Generate predictions as a CSV file at `/workdir/outputs/predictions.csv` with
  exactly two columns:
    - id: The game identifier (as string)
    - prediction: Performance bucket (integer from 0-49)

  Example:
  ```
  id,prediction
  1,23
  2,45
  3,12
  ```

  DATA SCHEMA:

  The training data (`/workdir/data/train.csv`) contains the following columns:

  **Required columns:**
    - id: Unique game identifier (integer)
    - player_name: Name of the player (string)
    - team_abbreviation: Player's team code (string, e.g., "LAL", "GSW")
    - age: Player's age at game time (float)
    - player_height: Height in cm (float)
    - player_weight: Weight in kg (float)
    - college: College attended (string, may be null)
    - country: Country of origin (string)
    - draft_year: Year drafted (string or integer, may be "Undrafted")
    - draft_round: Draft round (string or integer, may be "Undrafted")
    - draft_number: Draft pick number (string or integer, may be "Undrafted")
    - season: NBA season (string, e.g., "2015-16")
    - pts: Points scored (integer)
    - reb: Rebounds (integer)
    - ast: Assists (integer)
    - net_rating: Team net rating (float)
    - oreb_pct: Offensive rebound percentage (float)
    - dreb_pct: Defensive rebound percentage (float)
    - usg_pct: Usage percentage (float)
    - ts_pct: True shooting percentage (float)
    - ast_pct: Assist percentage (float)
    - performance_metric: Actual performance value (float, ONLY IN TRAINING DATA - DO NOT USE AS FEATURE)
    - performance_bucket: Target variable (integer 0-49, ONLY IN TRAINING DATA)

  The test data (`/tests/test.csv`) has the same schema but WITHOUT the performance_metric and
  performance_bucket columns.

  **Data Limitations:**
  - The dataset does NOT include: game dates, opponent identifiers, home/away indicators, or game ordering
  - Focus on features that can be derived from the available columns listed above

  CONTEXT:

  This task uses real NBA player game data with mixed numeric and categorical features.
  The dataset includes high-cardinality categorical variables (players, teams, colleges).

metadata:
  difficulty: hard
  category: machine-learning
  tags:
    - tabular-data
    - sports-analytics
    - player-performance
    - multi-class-classification
    - classical-ml
    - basketball-analytics
  time_limit: 7200
  memory_limit: 4096
  max_agent_timeout_sec: 7200
  expert_time_estimate_min: 180
  junior_time_estimate_min: 360

